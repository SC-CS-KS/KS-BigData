# What Is Data Collection?

数据采集服务，又叫 数据传输服务，数据采集服务，数据交换服务。

```text
数据采集的设计，几乎完全取决于数据源的特性
	毕竟数据源是整个大数据平台蓄水的上游，数据采集不过是获取水源的管道
在数据仓库的语境下
	ETL基本上就是数据采集的代表
但是在大数据平台下
	由于数据源具有更复杂的多样性，数据采集的形式也变得更加复杂而多样
		当然，业务场景也可能变得迥然不同
```

## Why

从大数据开发平台的角度来说
	很显然，是因为我们通常不能直接对线上业务系统所存储或生成的数据进行各种运算或检索处理
		组件技术架构是一方面原因，业务安全性隔离是另一方面原因。
在开发平台中处理完毕的数据
	有时候也并不能或着不适合在大数据开发平台的相关服务中直接使用
		需要反馈回线上的业务系统中，这个过程我们称为数据的回写或导出
即使在大数据开发平台自身的各种存储／计算／查询服务组件之间
	因为架构方案，读写方式，业务需求的不同，也可能存在数据的传输同步需求

## 典型的业务场景

为了提升业务处理的性能，同时又希望保留历史数据以备数据挖掘与分析
	实时将RDB的数据同步到HDFS中，让HDFS成为备份了完整数据的冗余存储
	在这种场景下，数据采集就仅仅是一个简单的同步，无需执行转换

数据源已经写入Kafka，需要实时采集数据
	在考虑流处理的业务场景，数据采集会成为Kafka的消费者
		就像一个水坝一般将上游源源不断的数据拦截住
		然后根据业务场景做对应的处理（例如去重、去噪、中间计算等），之后再写入到对应的数据存储中
	这个过程类似传统的ETL，但它是流式的处理方式，而非定时的批处理Job。
    
数据源为视频文件，需提取特征数据
	针对视频文件的大数据处理，需要在Extract阶段加载图片后
		然后根据某种识别算法，识别并提取图片的特征信息，并将其转换为业务场景需要的数据模型
	在这个场景下，数据提取的耗时相对较长，也需要较多的内存资源。
		如果处理不当，可能会成为整个数据阶段的瓶颈。

## 数据同步服务

主要处理的是不同系统组件之间的数据导入导出工作
	也就是输入和输出的数据源是异构的
		数据同步的目的是让数据可以适合业务需求的形式
			在不同的系统中用各自擅长的方式运转起来
	比如将DB的数据采集到Hive中来，将Hive中的数据导出给HBase之类
    
还有另外一种出于数据备份，或者负载均衡的目的而存在的数据同步场景
	他们的输入输出数据源往往是同构的
		属于系统自身功能实现的一部分
	比如MySQL的binlog主从同步复制机制