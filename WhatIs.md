# What Is Big Data?
```md
简而言之，它指的是数据集的数量和复杂程度如此之大，以至于它们无法通过传统的软件工具进行有效管理。
利用大数据的努力涉及许多处理数据创建，存储，检索和数据分析的新技术。
```

## 知识体系
* 数据采集
ETL工具负责将分布的、异构数据源中的数据如关系数据、平面数据文件等抽取到临时中间层后进行清洗、转换、集成，
最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘的基础。

* 数据存取
关系数据库、NOSQL、SQL等。

* 基础架构
云存储、分布式文件存储等。

* 数据处理
对于结构化数据处理
		MB级用excel,pandas,sqlite,access
		GB级用mysql,oracle,sql server,postgresql
		TB级用mongodb，greenplum
		PB级用hadoop,spark
		EB级自己想办法

* 统计分析
假设检验、显著性检验、差异分析、相关分析、T检验、方差分析、卡方分析、
	偏相关分析、距离分析、回归分析、简单回归分析、多元回归分析、逐步回归、回归预测与残差分析、
	岭回归、logistic回归分析、曲线估计、因子分析、聚类分析、主成分分析、因子分析、
	快速聚类法与聚类法、判别分析、对应分析、多元对应分析（最优尺度分析）、bootstrap技术等等。

* 数据挖掘
分类（Classification）、估计（Estimation）、预测（Prediction）、
		相关性分组或关联规则（Affinity grouping or association rules）、聚类（Clustering）、
	描述和可视化、Description and Visualization）、复杂数据类型挖掘(Text,Web ,图形图像，视频，音频等)

* 模型预测
预测模型、机器学习、建模仿真

* 结果呈现
云计算、标签云、关系图等

## 核心问题
大数据技术本质上无非解决4个核心问题
	
存储
    海量的数据怎样有效的存储
	主要包括hdfs、Kafka
计算
	海量的数据怎样快速计算
	主要包括MapReduce、Spark、Flink等
查询
    海量数据怎样快速查询
	主要为Nosql和OLAP
		Nosql主要包括Hbase、 Cassandra 等
		olap包括kylin、impla等
		其中Nosql主要解决随机查询，OLAP技术主要解决关联查询；
挖掘
    海量数据怎样挖掘出隐藏的知识
	也就是当前火热的机器学习和深度学习等技术
	包括TensorFlow、caffe、mahout等

## 特征

大规模
	据Gartner称，大规模可以被定义为“在本（地）机数据采集和处理技术能力不足以为用户带来商业价值
	当现有的技术能够针对性的进行改造后来处理这种规模的数据就可以说是一个成功的大数据解决方案。
	这种大规模的数据没将不仅仅是来自于现有的数据源
		同时也会来自于一些新兴的数据源
		例如常规（手持、工业）设备，日志，汽车等，当然包括结构化的和非结构化的数据。
多样性
	高度变异的信息资产，在生产和消费时不进行严格定义的包括多种形式、类型和结构的组合
	同时还包括以前的历史数据，由于技术的变革历史数据同样也成为多样性数据之一 
高效性
	来自不同源的数据到达的速度
		从各种设备，传感器和其他有组织和无组织的数据流都在不断进入IT系统
		由此，实时分析和对于该数据的解释（展示）的能力也应该随之增加。
	根据Gartner，高效性可以被定义如下：
		“高速的数据流I/O(生产和消费)，
			但主要聚焦在一个数据集内或多个数据集之间的数据生产的速率可变上”。
准确性
	或真实性或叫做精度是数据的另一个重要组成方面
	要做出正确的商业决策，当务之急是在数据上进行的所有分析必须是正确和准确（精确）的。
价值性
	大数据系统可以提供巨大的商业价值
	像电信，金融，电子商务，社交媒体等，已经认识到他们的数据是一个潜在的巨大的商机
		他们可以预测用户行为，并推荐相关产品，提供危险交易预警服务，等等

## 发展

### Google三本阵法修炼秘籍
《Google file system》
	论述了怎样借助普通机器有效的存储海量的大数据；
《Google MapReduce》
	论述了怎样快速计算海量的数据；
《Google BigTable》
	论述了怎样实现海量数据的快速查询；

### Hadoop
HDFS
	hdfs解决大数据的存储问题
MapReduce
	解决大数据的计算问题
Hbase
	解决大数据量的查询问题

### Hadoop不断衍生和进化各种分支流派

其中最激烈的当属计算技术，其次是查询技术。
	存储技术基本无太多变化，hdfs一统天下。
传统数据仓库派
	mapreduce修炼太复杂，老子不会编程，老子以前用sql吃遍天下，
		为了将这拨人收入门下，并降低大数据修炼难度
	遂出了hive，pig、impla等SQL ON Hadoop的简易修炼秘籍；
伯克利派
	说你MapReduce只重招数，内力无法施展，且不同的场景需要修炼不同的技术，太过复杂
	于是推出基于内力（内存）的《Spark》，意图解决所有大数据计算问题。
流式计算相关门派
	说你hadoop只能憋大招（批量计算），太麻烦
	于是出了SparkStreaming、Storm，S4等流式计算技术，能够实现数据一来就即时计算。
apache
	看各大门派纷争四起
	推出flink，想一统流计算和批量计算的修炼；

### Hadoop生态发展

MapReduce
Pig和Hive
	把脚本和SQL语言翻译成MapReduce程序，丢给计算引擎去计算，
		而你就从繁琐的MapReduce程序中解脱出来，用更简单更直观的语言去写程序了。
		Pig是接近脚本方式去描述MapReduce
		Hive则用的是SQL
Impala，Presto，Drill
	三个系统的核心理念是
		MapReduce引擎太慢，因为它太通用，太强壮，太保守
		我们SQL需要更轻量，更激进地获取资源，更专门地对SQL做优化，而且不需要那么多容错性保证
			因为系统出错了大不了重新启动任务，如果整个处理时间更短的话，比如几分钟之内
		这些系统让用户更快速地处理SQL任务，牺牲了通用性稳定性等特性
	这些系统，说实话，一直没有达到人们期望的流行度
Hive on Tez / Spark和SparkSQL
中低速数据处理
	底层HDFS，上面跑MapReduce/Tez/Spark，在上面跑Hive，Pig
	或者HDFS上直接跑Impala，Drill，Presto
那如果我要更高速的处理呢？
	Streaming(流)计算
		思路是
			如果要达到更实时的更新，我何不在数据流进来的时候就处理了?
		流计算很牛逼，基本无延迟，
			但是它的短处是，不灵活，你想要统计的东西必须预先知道，
			毕竟数据流过就没了，你没算的东西就无法补算了
		因此它是个很好的东西，但是无法替代上面数据仓库和批处理系统。
	Storm是最流行的流计算平台
独立的模块
	KV Store
		KV Store就是说，我有一堆键值，我能很快速滴获取与这个Key绑定的数据
		理念是
			基本无法处理复杂的计算，大多没法JOIN，也许没法聚合，没有强一致性保证
			但是丫就是快。极快。
		每个不同的KV Store设计都有不同取舍，有些更快，有些容量更高，有些可以支持更复杂的操作
	比如Cassandra，HBase，MongoDB以及很多很多很多很多其他的(多到无法想象)
特制的系统/组件
	Mahout是分布式机器学习库
	Protobuf是数据交换的编码和库
	ZooKeeper是高一致性的分布存取协同系统
调度系统
	有了这么多乱七八糟的工具，都在同一个集群上运转，大家需要互相尊重有序工作
		所以另外一个重要组件是，调度系统
	Yarn

## 应用场景

数据分析(狭义)与数据挖掘的本质是一样的
	都是从数据里面发现有价值的信息
		从而帮助商务/业务运营、改进产品以及帮助企业做更好的决策
		所以数据分析(狭义)与数据挖掘构成广义的数据分析
	同时，数据挖掘受到很多学科领域的影响，其中数据库、机器学习、统计学影响最大。
简言之，对数据挖掘而言，数据库提供数据管理技术，机器学习和统计学提供数据分析技术
	并且统计学主要通过机器学习来对数据挖掘发挥影响
	而机器学习和数据库则是数据挖掘的两大支撑技术。
从数据分析的角度来看，绝大多数数据挖掘技术都来自机器学习领域
	但机器学习研究往往并不把海量数据作为处理对象
	因此，数据挖掘要对机器学习算法进行一些改造，使得算法性能和空间占用达到实用的地步。
	此外，数据挖掘还有自身独特的内容，即关联分析。
模式识别和机器学习是什么关系呢？
	传统的模式识别的方法一般分为两种——统计方法和句法方法。
	句法分析一般是不可学习的，而统计分析却发展了不少机器学习的方法
	同时机器学习也给模式识别提供了数据分析技术方法。

